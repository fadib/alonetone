#!/usr/bin/env ruby

require File.expand_path('../config/environment', __dir__)

# Use VIPS as the variant processor.
Rails.application.config.active_storage.variant_processor = :vips

require 'optparse'
require 'parallel'
require 'ruby-progressbar'

module Storage
  class SkipRecord < StandardError
  end

  class VariantMigration
    attr_reader :blob
    attr_reader :variant_name
    attr_reader :paperclip_attachment
    attr_reader :storage_attachment
    attr_reader :options

    def initialize(
      blob,
      variant_name:,
      paperclip_attachment:,
      storage_attachment:,
      options:
    )
      @blob = blob
      @variant_name = variant_name
      @paperclip_attachment = paperclip_attachment
      @storage_attachment = storage_attachment
      @options = options
    end

    def storage_variant
      storage_attachment.variant(resize_to_limit: resize_to_limit)
    rescue ActiveStorage::InvariableError
      raise(
        Storage::SkipRecord,
        "Can't process variant because original is not an image: `#{original_s3_object.key}'"
      )
    end

    def original_s3_object
      @original_s3_object ||= ActiveStorage::Blob.service.send(
        :object_for, paperclip_attachment.path(variant_name)[1..]
      )
    end

    def copy_s3_object
      @copy_s3_object ||= ActiveStorage::Blob.service.send(
        :object_for, storage_variant.key
      )
    end

    def migrate
      return if copy_s3_object.exists?

      if original_s3_object.exists?
        Storage.copy_object(
          original_s3_object,
          copy_s3_object,
          content_type: blob.content_type,
          options: options
        )
      else
        process
      end
    end

    private

    def process
      begin
        storage_variant.processed unless options.dry_run
        options.logger.info(
          "Generated variant #{variant_name}"
        )
      rescue ActiveStorage::IntegrityError
        options.logger.info(
          "Failed to generate #{variant_name} because #{storage_variant.blob.key} " \
          "has the wrong checksum."
        )
        raise
      rescue ActiveStorage::FileNotFoundError
        options.logger.info(
          "Failed to generate #{variant_name} because #{storage_variant.blob.key} is " \
          "missing."
        )
      end
    end

    def resize_to_limit
      ImageVariant.resize_to_fit(variant_name)
    end
  end

  class RecordMigration
    attr_reader :record
    attr_reader :storage_attachment_names
    attr_reader :paperclip_attachment_name
    attr_reader :options

    def initialize(
      record,
      storage_attachment_names:,
      paperclip_attachment_name:,
      options:
    )
      @record = record
      @storage_attachment_names = storage_attachment_names
      @paperclip_attachment_name = paperclip_attachment_name
      @options = options
    end

    def storage_record
      # The Active Storage attachment is not always on the record where the Paperclip attachment
      # came from.
      case paperclip_attachment_name
      when 'pic'
        record.picable
      else
        record
      end
    end

    # Returns Paperclip::Attachment
    def paperclip_attachment
      @paperclip_attachment ||= record.send(paperclip_attachment_name)
    end

    def storage_attachment_name
      if storage_record.nil?
        raise(
          Storage::SkipRecord,
          "There is no storage record associated as `#{paperclip_attachment_name}'"
        )
      end
      storage_attachment_names.fetch(storage_record.class)
    end

    # Returns ActiveStorage attachable
    def storage_attachment
      @storage_attachment ||= storage_record.send(storage_attachment_name)
    end

    def original_filename
      record.send("#{paperclip_attachment_name}_file_name")
    end

    def original_byte_size
      record.send("#{paperclip_attachment_name}_file_size")
    end

    def original_content_type
      record.send("#{paperclip_attachment_name}_content_type")
    end

    def build_blob
      ActiveStorage::Blob.new(
        filename: original_filename,
        byte_size: original_byte_size,
        content_type: original_content_type,
        checksum: checksum
      )
    end

    def blob
      @blob ||= storage_attachment.attached? ? storage_attachment.blob : build_blob
    end

    def s3_object(variant_name)
      @objects ||= {}
      @objects[variant_name] ||= s3_object_variant_name(variant_name)
    end

    def s3_object_variant_name(variant_name)
      ActiveStorage::Blob.service.send(
        :object_for,
        paperclip_attachment.path(variant_name)[1..]
      )
    end

    def find_highest_quality_object
      variant_names.reverse.each do |variant_name|
        object = s3_object(variant_name)
        if object.exists?
          options.logger.info(
            "Using variant `#{variant_name}' because it's the largest."
          )
          return object
        end
      end
      nil
    end

    def find_original_s3_object
      variant_names.each do |variant_name|
        begin
          object = s3_object(variant_name)
          if object.content_length == original_byte_size
            options.logger.info(
              "Using variant `#{variant_name}' because it has the correct byte size."
            )
            return object
          end
        rescue  Aws::S3::Errors::NotFound
        end
      end
      nil
    end

    def paperclip_original_s3_object
      object = s3_object(paperclip_attachment.path)
      object if object.exists?
    end

    def determine_original_s3_object
      if paperclip_attachment_name == 'pic'
        paperclip_original_s3_object || find_original_s3_object || find_highest_quality_object
      else
        paperclip_original_s3_object
      end
    end

    def original_s3_object
      if defined?(@original_s3_object)
        @original_s3_object
      else
        @original_s3_object = determine_original_s3_object
        if @original_s3_object
          options.logger.info("Using object with key `#{@original_s3_object.key}'")
        end
        @original_s3_object
      end
    end

    def copy_s3_object
      @copy_s3_object ||= ActiveStorage::Blob.service.send(:object_for, blob.key)
    end

    def migrate
      unless storage_attachment.attached?
        attach_blob
      end
      # Copy over all variants in case we are working on a Pic.
      if paperclip_attachment_name == 'pic'
        copy_variants
      end
    rescue Storage::SkipRecord => error
      options.logger.info(error.message)
      return
    end

    private

    def variant_names
      ImageVariant::VARIANTS.keys
    end

    # A MD5 checksum of the file data is required Blob. Unfortunately that means we need to
    # fetch the entire file from storage so this might take a while for large MP3s.
    def checksum
      @checksum ||= Digest::MD5.base64digest(original_s3_object.get.body.string)
    end

    def attach_blob
      if original_s3_object.nil?
        raise(
          Storage::SkipRecord,
          "Skipping migration for `#{storage_attachment_name}' because original S3 object is " \
          "missing"
        )
      end

      # Perform copy and Blob creation in a transaction so it rolls back when something goes
      # wrong.
      record.class.transaction do
        Storage.copy_object(
          original_s3_object,
          copy_s3_object,
          content_type: blob.content_type,
          options: options
        )
        storage_attachment = storage_record.send(
          "build_#{storage_attachment_name}_attachment",
          name: storage_attachment_name ,
          blob: blob
        )
        unless options.dry_run?
          # Create Blob and Attachment based on the copied object.
          storage_attachment.save!
        end
      end
    end

    def copy_variants
      variant_names.each do |variant_name|
        Storage::VariantMigration.new(
          blob,
          variant_name: variant_name,
          paperclip_attachment: paperclip_attachment,
          storage_attachment: storage_attachment,
          options: options
        ).migrate
      end
    end
  end

  class Migration
    attr_reader :model
    attr_reader :storage_attachment_names
    attr_reader :options

    def initialize(model, storage_attachment_names:, options:)
      @model = model
      @storage_attachment_names = storage_attachment_names
      @options = options
    end

    def migrate
      options.logger.info("Migrate #{model.name} (#{paperclip_attachments_names.to_sentence})")
      paperclip_attachments_names.each do |paperclip_attachment_name|
        model.find_in_batches do |batch|
          Parallel.each(batch, in_threads: 4) do |record|
            # Don't re-use the connection from the main thread.
            ActiveRecord::Base.connection_pool.with_connection do
              migrate_record(record, paperclip_attachment_name: paperclip_attachment_name)
              options.progress.increment
            end
          end
        end
      end
    end

    private

    FILE_NAME_COLUMN_RE = /(.+)_file_name$/.freeze

    def paperclip_attachments_names
      model.column_names.map do |column_name|
        if match = FILE_NAME_COLUMN_RE.match(column_name)
          match[1]
        end
      end.compact
    end

    def migrate_record(record, paperclip_attachment_name:)
      Storage::RecordMigration.new(
        record,
        storage_attachment_names: storage_attachment_names,
        paperclip_attachment_name: paperclip_attachment_name,
        options: options
      ).migrate
    end
  end

  def self.options
    @options ||= begin
      options = OpenStruct.new(
        dry_run: false
      )
      def options.dry_run?
        dry_run
      end
      options
    end
  end

  # rubocop:disable AbcSize
  # rubocop:disable Metrics/BlockLength
  def self.option_parser
    OptionParser.new do |parser|
      parser.banner = "Usage: #{$0} [options]"

      parser.separator ""
      parser.separator "Options:"

      parser.on(
        "-d", "--dry-run",
        "Run without changes to S3 and database."
      ) do
        options.dry_run = true
      end

      parser.on_tail("-h", "--help", "Show this message") do
        options.run = false
        STDOUT.puts parser
        exit
      end
    end
  end
  # rubocop:enable Metrics/BlockLength
  # rubocop:enable AbcSize

  def self.logger_filename
    File.expand_path('../log/storage_migration.log', __dir__)
  end

  def self.migrate(argv, paperclip_models:, storage_attachment_names:)
    option_parser.parse!(argv)

    FileUtils.mkdir_p(File.dirname(logger_filename))
    options.logger = Logger.new(logger_filename)
    options.logger.info("Starting run")

    options.progress = ProgressBar.create(
      title: 'Migrating',
      format: '%t |%E | %B',
      total: paperclip_models.sum { |model| model.count }
    )

    paperclip_models.each do |model|
      Storage::Migration.new(
        model, storage_attachment_names: storage_attachment_names, options: options
      ).migrate
    end
  rescue Interrupt
  ensure
    options.progress.finish
  end

  def self.copy_object(original, copy, content_type:, options:)
    options.logger.info("COPY #{original.key} TO #{copy.key} (#{content_type})")
    unless options.dry_run?
      # Using copy_from to work around a bug in the AWS SDK S3 library. Note that the library
      # performs all types of network optimizations and retries for us behind the scenes.
      copy.copy_from(original, content_type: content_type)
    end
  end
end

Storage.migrate(
  ARGV,
  paperclip_models: [
    Pic,
    Asset,
    Greenfield::AttachedAsset,
    Greenfield::PlaylistDownload
  ],
  storage_attachment_names: {
    User => 'avatar_image',
    Playlist => 'cover_image',
    Asset => 'audio_file',
    Greenfield::AttachedAsset => 'audio_file',
    Greenfield::PlaylistDownload => 'zip_file'
  }
)
